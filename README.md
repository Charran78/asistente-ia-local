# asistente-ia-local
Interfaz web para interactuar con modelos de lenguaje a travÃ©s de Ollama, construida con Gradio.

Estructura del proyecto

asistente-ia-local/
â”œâ”€â”€ asistente_ia.py      # CÃ³digo principal de la aplicaciÃ³n
â”œâ”€â”€ requirements.txt     # Dependencias de Python
â”œâ”€â”€ README.md           # Este archivo
â””â”€â”€ chat_history.db     # Base de datos (se crea automÃ¡ticamente)

## CaracterÃ­sticas

- Interfaz web amigable con Gradio
- Historial de conversaciones persistente en SQLite
- Soporte para diferentes modelos de Ollama
- Ajuste de parÃ¡metros como temperatura
- VisualizaciÃ³n del historial de conversaciones

## InstalaciÃ³n

1. Clona el repositorio:
```bash
git clone https://github.com/charran78/asistente-ia-local.git
cd asistente-ia-local

 ### ğŸ¤– Acerca de este Asistente

            Este es un asistente de IA local construido con:
            - **Gradio**: Para la interfaz web
            - **Ollama**: Para ejecutar modelos localmente
            - **Gemma2:2B**: Modelo de lenguaje de Google

            ### ğŸš€ InstalaciÃ³n y Uso

            1. Instala Ollama desde [ollama.com](https://ollama.com/)
            2. Descarga el modelo: `ollama pull gemma2:2b`
            3. Ejecuta este script: `python asistente_ia.py`

            ### ğŸ“¦ Dependencias

            Instala las dependencias con:
            ```bash
            pip install -r requirements.txt
            ```

            ### ğŸ“ Licencia

            Este proyecto estÃ¡ bajo la Licencia MIT.

            ### ğŸ‘¨â€ğŸ’» Autor

            Pedro MencÃ­as - 2025

            ### ğŸ“§ Contacto

            [beyond.digital.web@gmail.com](mailto:beyond.digital.web@gmail.com)

            Hecho en Asturias con ğŸ’“ y {miles de errores}
            """)

